[{"content":"基本认识（1）  明确使用TensorFlow作为机器学习的框架，建立机器学习的基本认识\n TensorFlow部署  在笔记本电脑里安装Miniconda； 在环境变量中添加所安装Miniconda的相关路径； 将下载源改为清华镜像； 新建一个python3.7的环境，命名为tf； 在tf环境中用conda install安装TensorFlow  机器学习的基本认识 学习算法的目标可以理解为，在整个数据集上获取经验。那么深度学习采用的是多层神经网络来获取这个经验。为了实现这个目的，我们需要将数据集分成：\n 训练集 验证集 测试集  算法的具体做法就是在训练集上，不断迭代调整参数，让输出结果与实际结果越来越接近。\n用验证集，对训练集的模型进行测试，得到准确率，这时候会有遇到训练集上表现良好，但是验证集得到的结果准确率很低等问题（欠拟合或者过拟合）。这就需要对模型的策略、超参数等进行调整。\n测试集，就是用完全没有经过训练的数据集对模型进行最终的性能评价。\n","date":"2020-09-23","permalink":"https://york-yanyu.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1/","tags":["Deep Learning"],"title":"深度学习1"},{"content":"基本认识（2） 此篇用于统一思想。\n","date":"2020-09-23","permalink":"https://york-yanyu.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2/","tags":["Deep Learning"],"title":"深度学习2"},{"content":"学习Scrapy爬虫 1 新的起点，我要学习一些编程知识。没有野心，并没有想要以此为生的念头。我还是本本分分做我的工作。但是编程是不能绕过去的技能，无论以后想要干什么。因此我想重新上路，重新对待编程。\n第一个目标，爬虫技能。\n日志第一天：\n  安装anoconda，并且卸载了之前笔记本里的python；\n  所有包都从清华镜像下载；\n  试图安装scrapy，报两个包不兼容的错，先更新所有的包，然后再试；\n  成功安装scrapy\n  2  官网的tutorial，第一个spider；  ![](D:\\CSProject\\BlogManagement\\reference\\Image [2].png)\n scrapy框架的理解  你写一个Spyder，告诉他起始网站是啥，并且写一个parse方法，它返回你要的信息，然后找到下一个网址是什么，让爬虫继续爬下去。\n如何parse一个网站的内容，有两个方法，一个是css，一个是xpath，对于文本信息，没有办法的，必须熟悉正则表达式。\n 今天的基本操作：   新建一个project\u0026mdash;scrapy startproject name 爬一个网址\u0026mdash;scrapy shell \u0026ldquo;url\u0026quot;\u0026mdash;后续可以在shell里面操作这个网址所对应的网站的内容，用 response response.css(\u0026hellip;) response.xpath(\u0026hellip;) 在project里面运行\u0026mdash;scrapy crawl name (通常是自己起的名字，参考下个例子) 没有project直接运行\u0026mdash;scrapy runspider name.py ![](D:\\CSProject\\BlogManagement\\reference\\Image [4].png) 几个必备的元素：1. name2. start_urls3. parse函数 ![](D:\\CSProject\\BlogManagement\\reference\\Image [5].png) settings里面可以定义useragent  3 https://blog.csdn.net/qq_31082427/article/details/84987723\n以上的链接告诉我如何使用xpath helper插件，来定位或者找到网页中某个或某类元素的xpath，通过xpath可以准备处理爬虫的网页数据。\n![](D:\\CSProject\\BlogManagement\\reference\\Image [7].png)\n一个完整的spider程序，包含：\n 爬虫名 入口URL 解析方法：   提取哪些信息（通过xpath精确定位，还需要设置获取文本，图片等） 获取以后需要传送给pipeline （yield一下） 获取下一个爬取的链接，深入爬取数据  这里面的关键是，如何拆分网站的结构，然后利用xpath精确定位到自己想要的区域，再进行细分得到区域里面单门的信息，包含有文字，图片，视频，链接，基本上就是这些元素。\n当然还可以进一步从文本信息中，提取自己想要的部分，这应该是先把这些提取出来，再利用字符串的一些算法，来对其进行进一步的处理。我觉得这里面会涉及：\n 字符串的传统算法 正则表达式  最终可以将数据生成一张表，但是这样是不好的。这涉及数据库和excel的区别。为此，必须会一种数据库，这里和python一起，我再来学习一下MongoDB。\nhttps://www.cnblogs.com/cq146637/p/8082163.html\n这个链接告诉我如何用python和MongoDB数据库进行交互。\n4  在笔记本上上完成了mongodb的安装，并且通过创建配置文件和log文件，成功添加了mongodb的服务，现在设置为自动启动。 接下来将通过pymongo来连接并且操作mongodb数据库。https://www.cnblogs.com/nixingguo/p/7260604.html这个网站介绍一些基本的操作  ![](D:\\CSProject\\BlogManagement\\reference\\Image [8].png)\nMongoDB中的一些概念！\n![](D:\\CSProject\\BlogManagement\\reference\\Image [9].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [10].png)\n5  database 库 collection 相当于表 document 相当于一行数据 field 相当于一列数据 indexprimary key  一些常用的命令，或者数据常用的操作：\n  创建数据库 use DATABASE_NAME (切换数据库也用这个命令)\n  查看所有数据库 show dbs （刚创建的数据库，需要插入一些数据才会显示）\n  MongoDB 中默认的数据库为 test，如果你没有创建新的数据库，集合将存放在 test 数据库中。\n  删除数据库 db.dropDatabase()\n  创建集合 db.createCollection(name, options) (option可以包括以下的参数) ![](D:\\CSProject\\BlogManagement\\reference\\Image [11].png)不需要特别创建集合，可以自动生成 db.mycol2.insert({\u0026ldquo;name\u0026rdquo; : \u0026ldquo;菜鸟教程\u0026rdquo;})\n  删除集合 db.collection.drop() db.collection_name.drop()\n  插入文档（document）db.COLLETION_NAME.insert(document)\n  这里有必要提一下，document是指字典数据，就是键-\u0026gt;值，键-\u0026gt;值，这样的形式。\n 更新数据 db.collection.update() ![](D:\\CSProject\\BlogManagement\\reference\\Image [12].png)  6 MongoDB的副本集，分片是比较核心的内容，基础的部分是前面的关于数据库的操作。存储，更新，查询等等。 MongoDB需要定义host，port，然后是数据库的名字，collection的名字：\n  host\n  port\n  db_name\n  collection_name\n  我觉得框架的含义，就是首先将一件事情分成各个模块，模块之间相对来说比较独立，这是第一步； 然后就是保证各个模块之间高效的通信，这是比较难的一步；\n7 middleware中间件:\n 可以设置代理ip，运用代理ip爬取数据，达到隐藏自己的目的。 还可以设置user-agent，从一个列表中随机抽取一个作为使用的user_agent，也可以达到隐藏自己的目的。  extract() 和 extract_first(): 前者提取一个数组 后者提取第一个匹配的数据\n![](D:\\CSProject\\BlogManagement\\reference\\Image [13].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [14].png)\n安装了可视化工具，方便查看mongodb的数据\npython中的类\n![](D:\\CSProject\\BlogManagement\\reference\\Image [15].png)\npython中的包：\n![](D:\\CSProject\\BlogManagement\\reference\\Image [16].png)\n其中的\n__init__.py\r 目录只有包含一个叫做 _init_.py 的文件才会被认作是一个包，主要是为了避免一些滥俗的名字（比如叫做 string）不小心的影响搜索路径中的有效模块。\n#爬取豆瓣top250上的电影信息#解决了字符串的正则表达式的一个小问题，就是将介绍里面的主演和演员和其他信息分开，并且转变成字符串。\n8 提一个新的问题，怎么爬取豆瓣top250的电影图片，并且将每张图片的名字命名为该电影的名称！ 另外，发现一个特别好的编程教程，廖雪峰的教程，深入浅出，符合我的认知。 找到了url和名称，但是不知道怎么用pipeline将图片保存下来，保存在什么地方？\n9 弄明白了怎么爬取图片，并且怎么将图片命名。 提一个更复杂的问题，如何爬取top250各自所有的正式海报的原图？\n10 ![](D:\\CSProject\\BlogManagement\\reference\\Image [17].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [18].png)\n11 ![](D:\\CSProject\\BlogManagement\\reference\\Image [19].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [20].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [21].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [22].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [23].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [24].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [25].png)\n12 ![](D:\\CSProject\\BlogManagement\\reference\\Image [26].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [27].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [28].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [29].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [30].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [31].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [32].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [34].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [35].png)\n![](D:\\CSProject\\BlogManagement\\reference\\Image [36].png)\n","date":"2020-05-11","permalink":"https://york-yanyu.github.io/post/%E5%AD%A6%E4%B9%A0scrapy%E7%88%AC%E8%99%AB/","tags":["Scrapy爬虫"],"title":"Scrapy"}]